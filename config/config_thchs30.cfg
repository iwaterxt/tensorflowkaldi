[directories]
#directory where the training data will be retrieved
train_data =/home/tao/Works/kaldi/egs/thchs30/s5/data/mfcc/train
#directory where the dev data will be retrieved
dev_data =/home/tao/Works/kaldi/egs/thchs30/s5/data/mfcc/dev
#directory where the testing data will be retrieved
test_data =/home/tao/Works/kaldi/egs/thchs30/s5/data/mfcc/test
#directory where the training features will be stored and retrieved
train_features =/home/tao/Works/kaldi/egs/thchs30/s5/data/fbank/train
#directory where the dev features will be stored and retrieved
dev_features =/home/tao/Works/kaldi/egs/thchs30/s5/data/fbank/dev
#directory where the testing features will be stored and retrieved
test_features =/home/tao/Works/kaldi/egs/thchs30/s5/data/fbank/test
#directory where the language model will be retrieved
language =/home/tao/Works/kaldi/egs/thchs30/s5/data/graph/lang
#directory where the language model will be retrieved that is used to create the decoding graph
language_test =/home/tao/Works/kaldi/egs/thchs30/s5/data/graph/lang
#directory where the all the data from this experiment will be stored (logs, models, ...)
expdir =/home/tao/Works/kaldi/egs/thchs30/s5/expdir
#path to the KALDI egs folder

KALDI_egs =/home/tao/Works/kaldi/egs/thchs30/s5
hdf5_train =/home/tao/Works/kaldi/egs/thchs30/s5/expdir/hdf5_train.h5
hdf5_dev =/home/tao/Works/kaldi/egs/thchs30/s5/expdir/hdf5_dev.h5



[general]
# number of jobs for KALDI
num_jobs = 4
#command used for KALDI
cmd =/home/tao/Works/kaldi/egs/thchs30/s5/utils/run.pl

[gmm-features]
#name of the features
name = 13mfcc
#feature type options: mfcc, fbank and ssc
type = mfcc
#the dynamic information that is added to the features, options are nodelta, delta and ddelta
dynamic = nodelta
#length of the sliding window (seconds)
winlen = 0.025
#step of the sliding window (seconds)
winstep = 0.01
#number of fbank filters
nfilt = 23
#number of fft bins
nfft = 512
#low cuttof frequency
lowfreq = 0
#hight cutoff frequency, if -1 set to None
highfreq = -1
#premphesis
preemph = 0.97
#include energy in features
include_energy = False
#snip the edges for sliding window
snip_edges = True
#mfcc option: number of cepstrals
numcep = 13
#mfcc option: cepstral lifter (used to scale the mfccs)
ceplifter = 22

[dnn-features]
#name of the features. If you want to use the GMM features, give it the same name
name = 40fbank
#feature type options: mfcc, fbank and ssc
type = fbank
#the dynamic information that is added to the features, options are nodelta, delta and ddelta
dynamic = nodelta
#length of the sliding window (seconds)
winlen = 0.025
#step of the sliding window (seconds)
winstep = 0.01
#number of fbank filters
nfilt = 40
#number of fft bins
nfft = 512
#low cuttof frequency
lowfreq = 0
#hight cutoff frequency, if -1 set to None
highfreq = -1
#premphesis
preemph = 0.97
#include energy in features
include_energy = False
#snip the edges for sliding window
snip_edges = True

[mono_gmm]
#name of the monophone gmm
name = mono_gmm

[tri_gmm]
#name of the triphone gmm
name = tri_gmm
#triphone gmm parameters (KALDI)
num_leaves = 2000
tot_gauss = 10000

[lda_mllt]
#name of the LDA+MLLT GMM
name = lda_mllt_gmm
#size of the left and right context window
context_width = 3
#lda_mllt gmm parameters (KALDI)
num_leaves = 2500
tot_gauss = 15000

[nnet]
#name of the neural net
name = 1024_4_relu
#name of the gmm model used for the alignments
gmm_name = lda_mllt_gmm
#size of the left and right context window
context_width = 5
#number of neurons in the hidden layers
num_hidden_units = 1024
#number of hidden layers
num_hidden_layers = 4
#the network is initialized layer by layer. This parameters determines the frequency of adding layers. Adding will stop when the total number of layers is reached. Set to 0 if no layer-wise initialisation is required
add_layer_period = 0
#starting step, set to 'final' to skip nnet training
starting_step = 0
#if you're using monophone alignments, set to True
monophone = False
#nonlinearity used currently supported: relu, tanh, sigmoid
nonlin = relu
#if you want to do l2 normalization after every layer set to 'True'
l2_norm = False
#if you want to use dropout set to a value smaller than 1
dropout = 1
#Flag for using batch normalisation
batch_norm = False
#initial learning rate of the neural net
initial_learning_rate = 0.01
#exponential weight decay parameter
learning_rate_decay = 1
#halve_learning_rate
halve_learning_rate = 0
#size of the minibatch (#utterances)
minibatch_size = 128
#to limit memory ussage (specifically for GPU) the batch can be devided into
#even smaller batches. The gradient will be calculated by averaging the
#gradients of all these mini-batches. This value is the size of these
#mini-batches in number of utterances. For optimal speed this value should be
#set as high as possible without exeeding the memory. To use the entire batch
#set to -1
numutterances_per_block = 200
#frequency of evaluating the validation set
#if you want to adapt the learning rate based on the validation set, set to True
#you can visualise the progress of the neural net with tensorboard
visualise = False
#if the ce impr is less than start_halving_impr start havling learning rate 
start_halving_impr=0.01
#if the ce impr is less than end_halving_impr end the training process.
end_halving_impr=0.001
#

epoch = 0
#max number of epoch
max_epoch = 20
#momentum param
momentum = 0.9
#clip gradient
clip_grad = 50
#L1 regularization
l1_penalty=1e-6
#L2 regularization
l2_penalty=1e-6
#weigth initialization
weight_init = uniform

#splice 
splice=5
#splice_step
splice_step=1
#norm_vars
norm_vars = true
#delta_order
delta_order = 0
#number of gpu
n_gpus = 1

